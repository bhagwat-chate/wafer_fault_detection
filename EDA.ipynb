{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63e819f",
   "metadata": {},
   "source": [
    "# Sensor Fault Detection\n",
    "The inputs of various sensors for different wafers have been provided. In electronics, a wafer (also called a slice or substrate) is a thin slice of semiconductor used for the fabrication of integrated circuits. The goal is to build a machine learning model which predicts whether a wafer needs to be replaced or not(i.e., whether it is working or not) based on the inputs from various sensors. There are two classes: +1 and -1.\n",
    "\n",
    "    •\t+1 means that the wafer is in a working condition and it doesn’t need to be replaced.\n",
    "    •\t-1 means that the wafer is faulty and it needs to be replaced. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fc765",
   "metadata": {},
   "source": [
    "#### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,roc_curve,confusion_matrix\n",
    "from sklearn.model_selection import  train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from catboost import CatBoostClassifier\n",
    "# from imblearn.combine import SMOTETomek\n",
    "import imbalanced-learn as imbalance\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f4225",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840996d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flage = 0\n",
    "path = \"wafer\\\\data_ingestion\\\\Training_raw_files_validated\\\\Good_Raw\"\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    temp = pd.read_csv(path+'\\\\'+f)\n",
    "\n",
    "    if flage == 0:\n",
    "        flage=1\n",
    "        data = temp\n",
    "    else:\n",
    "        data = data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features: \", data.shape[1])\n",
    "print(\"Unique features: \", data['Good/Bad'].unique())\n",
    "print(\"Number of features: \"+str(data['Good/Bad'].nunique()) + \", It's binary classification\")\n",
    "print(\"Class distribution: \\n\", data['Good/Bad'].value_counts(), \" it's perfectly imbalance classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2536d",
   "metadata": {},
   "source": [
    "#### Categorical and numerical feature count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac3e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Numerical feature count: \",len([f for f in data.columns if data[f].dtypes != 'O']))\n",
    "print(\"Numerical feature: \",[f for f in data.columns if data[f].dtypes != 'O'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count of numerical features: \", len([f for f in data.columns if data[f].dtypes == 'O']))\n",
    "print(\"Numerical features: \", [f for f in data.columns if data[f].dtypes == 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafer_name = data['Wafer'].nunique()\n",
    "data.drop(['Wafer'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86abf623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdc5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e7a823e",
   "metadata": {},
   "source": [
    "### As this is sensor data, interpretation of data is not required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00403ebc",
   "metadata": {},
   "source": [
    "#### Checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a33caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Missing values count for each column\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "missing = data.isna().sum().div(data.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)\n",
    "\n",
    "ax.bar(missing.index, missing.values.T[0])\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"Percentage missing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a55531",
   "metadata": {},
   "source": [
    "#### Droping columns which has >70% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = missing[missing[0] > 70]\n",
    "dropcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd961dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(list(dropcols.index), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6fe42",
   "metadata": {},
   "source": [
    "#### Check the percentage os missing values after droping features with more than 70% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = data.isna().sum()\n",
    "total_cells = np.product(data.shape)\n",
    "total_missing_values = missing_values_count.sum()\n",
    "print(f\"Percentage of total missing cells in the data {round((total_missing_values/total_cells) * 100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a96b9",
   "metadata": {},
   "source": [
    "#### Visualization of unique values in label feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea391dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data[data['Good/Bad']==+1].shape[0]\n",
    "neg = data[data['Good/Bad']==-1].shape[0]\n",
    "sns.catplot(data=data, x=\"Good/Bad\", kind=\"count\", palette=\"winter_r\", alpha=.6)\n",
    "plt.title(\"Label Count: Class +1: {v1}, Class -1: {v2}\".format(v1=pos, v2=neg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db44024",
   "metadata": {},
   "source": [
    "#### Report\n",
    "\n",
    "- The target classes are highly imbalanced\n",
    "- Class imbalance is a scenario that arises when we have unequal distribution of class in a dataset i.e. the no. of data points in the negative class (majority class) very large compared to that of the positive class (minority class)\n",
    "- If the imbalanced data is not treated beforehand, then this will degrade the performance of the classifier model.\n",
    "    Hence we should handle imbalanced data with certain methods.\n",
    "\n",
    "#### How to handle Imbalance Data ?\n",
    "\n",
    "- Resampling data is one of the most commonly preferred approaches to deal with an imbalanced dataset. There are broadly two types of methods for this i) Undersampling ii) Oversampling. In most cases, oversampling is preferred over undersampling techniques. The reason being, in undersampling we tend to remove instances from data that may be carrying some important information.\n",
    "- SMOTE: Synthetic Minority Oversampling Technique SMOTE is an oversampling technique where the synthetic samples are generated for the minority class.\n",
    "- Hybridization techniques involve combining both undersampling and oversampling techniques. This is done to optimize the performance of classifier models for the samples created as part of these techniques.\n",
    "- It only duplicates the data and it won't add and new information. Hence we look at some different techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a60c7",
   "metadata": {},
   "source": [
    "#### Create Functions for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7fc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(true, predicted):\n",
    "    '''\n",
    "    This function takes in true values and predicted values\n",
    "    Returns: Accuracy, F1-Score, Precision, Recall, Roc-auc Score\n",
    "    '''\n",
    "    acc = accuracy_score(true, predicted) # Calculate Accuracy\n",
    "    f1 = f1_score(true, predicted) # Calculate F1-score\n",
    "    precision = precision_score(true, predicted) # Calculate Precision\n",
    "    recall = recall_score(true, predicted)  # Calculate Recall\n",
    "    roc_auc = roc_auc_score(true, predicted) #Calculate Roc\n",
    "    return acc, f1 , precision, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost of the model as per data description\n",
    "def total_cost(y_true, y_pred):\n",
    "    '''\n",
    "    This function takes y_ture, y_predicted, and prints Total cost due to misclassification\n",
    "   \n",
    "    '''\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    cost = 10*fp + 500*fn\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which can evaluate models and return a report \n",
    "def evaluate_models(X, y, models):\n",
    "    '''\n",
    "    This function takes in X and y and models dictionary as input\n",
    "    It splits the data into Train Test split\n",
    "    Iterates through the given model dictionary and evaluates the metrics\n",
    "    Returns: Dataframe which contains report of all models metrics with cost\n",
    "    '''\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    cost_list=[]\n",
    "    models_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for i in range(len(list(models))):\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, y_train) # Train model\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Training set performance\n",
    "        model_train_accuracy, model_train_f1,model_train_precision,\\\n",
    "        model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)\n",
    "        train_cost = total_cost(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "        # Test set performance\n",
    "        model_test_accuracy,model_test_f1,model_test_precision,\\\n",
    "        model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)\n",
    "        test_cost = total_cost(y_test, y_test_pred)\n",
    "\n",
    "        print(list(models.keys())[i])\n",
    "        models_list.append(list(models.keys())[i])\n",
    "\n",
    "        print('Model performance for Training set')\n",
    "        print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "        print('- F1 score: {:.4f}'.format(model_train_f1)) \n",
    "        print('- Precision: {:.4f}'.format(model_train_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_train_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
    "        print(f'- COST: {train_cost}.')\n",
    "\n",
    "        print('----------------------------------')\n",
    "\n",
    "        print('Model performance for Test set')\n",
    "        print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
    "        print('- F1 score: {:.4f}'.format(model_test_f1))\n",
    "        print('- Precision: {:.4f}'.format(model_test_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_test_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
    "        print(f'- COST: {test_cost}.')\n",
    "        cost_list.append(test_cost)\n",
    "        print('='*35)\n",
    "        print('\\n')\n",
    "        \n",
    "    report=pd.DataFrame(list(zip(models_list, cost_list)), columns=['Model Name', 'Cost']).sort_values(by=[\"Cost\"])\n",
    "        \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216eff9",
   "metadata": {},
   "source": [
    "#### Plot distribution of all independent numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_features = [feature for feature in data.columns if data[feature].dtype != 'O']\n",
    "\n",
    "# plt.figure(figsize=(15, 100))\n",
    "# for i, col in enumerate(numeric_features):\n",
    "#     plt.subplot(60, 3, i+1)\n",
    "#     sns.distplot(x=data[col], color='indianred')\n",
    "#     plt.xlabel(col, weight='bold')\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f889a6",
   "metadata": {},
   "source": [
    "#### Report\n",
    "\n",
    "- As per the above plot most of the features are not normally distributed.\n",
    "- Transformation of data is not of prime importance since it is a classification problem.\n",
    "- Interpreting each and every column is not necessary as this is sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14380cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b03c44",
   "metadata": {},
   "source": [
    "#### Evaluate models based on experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Good/Bad\", axis=1)\n",
    "y = data['Good/Bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f428fba",
   "metadata": {},
   "source": [
    "#### Manually encode label feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= y.replace({-1: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a7f31",
   "metadata": {},
   "source": [
    "## Experiment: 1 - KNN Imputer for NULL values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3408f6",
   "metadata": {},
   "source": [
    "#### Why Robust scaler and not Standard scaler?\n",
    "\n",
    "- Scaling the data using Robust scaler\n",
    "- Since most of the independent variables are not normally distributed we cannot use Standardscaler\n",
    "\n",
    "#### Why Robust Scaler and not Minmax?\n",
    "\n",
    "- Because most of the feature has outliers. So Minmax will scale data according to Max values which is outlier.\n",
    "- This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "robustscaler = RobustScaler()\n",
    "X1 = robustscaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858b42a",
   "metadata": {},
   "source": [
    "#### Why KNN Imputer?\n",
    "\n",
    "- KNNImputer by scikit-learn is a widely used method to impute missing values. It is widely being observed as a replacement for traditional imputation techniques.\n",
    "- KNNImputer helps to impute missing values present in the observations by finding the nearest neighbors with the Euclidean distance matrix.\n",
    "- Here we Iterates through different K values and get accuracy and choose best K values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39fee9",
   "metadata": {},
   "source": [
    "#### Finding the optimal n_neighbour value for KNN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "# define imputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "strategies = [str(i) for i in [1,3,5,7,9]]\n",
    "for s in strategies:\n",
    "    pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m', LogisticRegression())])\n",
    "    scores = cross_val_score(pipeline, X1, y, scoring='accuracy', cv=2, n_jobs=-1)\n",
    "    results.append(scores)\n",
    "    print('n_neighbors= %s || accuracy (%.4f)' % (s , mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d5e2f",
   "metadata": {},
   "source": [
    "#### We can observe n_neighbors=3 able to produce highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be4e83",
   "metadata": {},
   "source": [
    "#### Pipeline for KNN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# Fit the KNN imputer with selected K-value\n",
    "knn_pipeline = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=3)),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785301e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn =knn_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28a73d",
   "metadata": {},
   "source": [
    "### Handling Imbalanced data\n",
    "- SMOTE+TOMEK is one of such a hybrid technique that aims to clean overlapping data points for each of the classes distributed in sample space.\n",
    "\n",
    "- This method combines the SMOTE ability to generate synthetic data for minority class and Tomek Links ability to remove the data that are identified as Tomek links from the majority class\n",
    "\n",
    "#### To add new data of minority class\n",
    "\n",
    "- Choose random data from the minority class.\n",
    "- Calculate the distance between the random data and its k nearest neighbors.\n",
    "- Multiply the difference with a random number between 0 and 1, then add the result to the minority class as a synthetic sample.\n",
    "- Repeat step number 2–3 until the desired proportion of minority class is met.\n",
    "- To remove the tomek links of the majority class\n",
    "- Choose random data from the majority class.\n",
    "- If the random data’s nearest neighbor is the data from the minority class (i.e. create the Tomek Link), then remove the Tomek Link.\n",
    "- This is method instead of adding duplicate data it synthesises the new data based on the already avalialble classes. Hence we choose this as our imputer method for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9eaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority',n_jobs=-1)\n",
    "\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority',n_jobs=-1)\n",
    "\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_knn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d99b2",
   "metadata": {},
   "source": [
    "#### Initialize Default Models in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary which contains models for experiment\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "     \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"XGBClassifier\": XGBClassifier(), \n",
    "     \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cab25",
   "metadata": {},
   "source": [
    "#### Fit KNN imputed data for models in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323974e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_knn = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae71581",
   "metadata": {},
   "source": [
    "### Report for KNN Imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b825dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345ea39",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "\n",
    "- For the Experiment 1: Knn imputer has XGBoost classifier as the best Model\n",
    "- Proceeding with further experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4d1b3",
   "metadata": {},
   "source": [
    "### Experiment: 2 = Simple Imputer with Strategy Median\n",
    "- SimpleImputer is a class in the sklearn.impute module that can be used to replace missing values in a dataset, using a variety of input strategies.\n",
    "- Here we use SimpleImputer can also be used to impute multiple columns at once by passing in a list of column names. SimpleImputer will then replace missing values in all of the specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b043087",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "# Fit the Simple imputer with strategy median\n",
    "median_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('RobustScaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d4c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit X with median_pipeline\n",
    "X_median = median_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "smt = SMOTETomek(random_state=42,sampling_strategy='minority')\n",
    "# Fit the model to generate the data.\n",
    "X_res, y_res = smt.fit_resample(X_median, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbe8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the models\n",
    "report_median = evaluate_models(X_res, y_res, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4ad73",
   "metadata": {},
   "source": [
    "### Report for MICE Imputer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af3ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2963544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44047b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89a1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e70e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3bdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f85be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25cef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85f938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bff148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c542d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
